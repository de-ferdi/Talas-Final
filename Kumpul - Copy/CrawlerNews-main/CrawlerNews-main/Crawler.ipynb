{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarmaCahya/CrawlerNews/blob/main/test_Crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trTDb_OC8hP-"
      },
      "source": [
        "# Web Scraping and Data Collection from News Websites\n",
        "\n",
        "This notebook demonstrates how to scrape news articles from various websites using Python libraries such as `requests`, `BeautifulSoup`, and `Selenium`. The collected data is then stored in a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note:** For optimal experience and ease of use, it is recommended to run this notebook using Visual Studio Code (VSC).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PNmkkGY_N1j"
      },
      "source": [
        "## Install Libraries\n",
        "First, we need to install the required libraries. You can install them by running the following commands:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUW-C6KugHFf",
        "outputId": "83d99fd2-1d0e-46c5-e4cb-0b9ef96f013e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.0-py3-none-any.whl (475 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.7/475.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.6.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.22.0 trio-0.26.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xo2Xv6Lat35"
      },
      "source": [
        "# Crawling news data on several websites in a general way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L64jBHNWhK6S",
        "outputId": "fe0e8a22-e811-45d4-c460-7b52e5998b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling page 1\n",
            "Found 10 articles on page: https://www.cnnindonesia.com/politik/indeks/4\n",
            "Appended article: Tito Sindir Bupati Petahana Susah Dicari: Sibuk Cari Tiket Pilkada\n",
            "Appended article: Tito Ungkap 10 Pj Kepala Daerah Mundur untuk Maju Pilkada 2024\n",
            "Appended article: Rencana Ubah Wantimpres Jadi DPA, DPR Tegaskan Takkan Ubah Fungsi\n",
            "Appended article: Pemerintah Mulai Susun Draf RUU TNI, Polri, Imigrasi, dan Kementerian\n",
            "Appended article: Pansus Haji DPR, Momen Ungkap Dugaan Pelanggaran Pemerintah\n",
            "Appended article: Cak Imin Pastikan Pansus Angket Haji Jalan di Masa Reses DPR\n",
            "Appended article: IDI Tak Tolak Dokter Asing tapi Desak Pemerintah Prioritaskan WNI\n",
            "Appended article: Paripurna Sahkan RUU Kepariwisataan Jadi Usul Inisiatif DPR\n",
            "Appended article: Paripurna DPR Resmi Sahkan RUU Sumber Daya Alam Hayati jadi UU\n",
            "Appended article: Airlangga Ungkap Rencana Kaesang Akan Bertamu ke Golkar Kamis Esok\n",
            "Scraped 10 articles from CNN Indonesia\n",
            "Crawling page 1\n",
            "Found 20 articles on page: https://news.detik.com/berita/indeks\n",
            "Appended article: Gempa M 5,8 Terjadi di Enggano Bengkulu\n",
            "Appended article: Sempat Dicopot, Prof Budi Santoso Resmi Dilantik Lagi sebagai Dekan FK Unair\n",
            "Appended article: Giliran Pelapor Ghatan Diproses Kasus Pengancaman\n",
            "Appended article: Kenneth DPRD Jakarta Minta Pemprov Segera Isi Posisi PJLP yang Kosong\n",
            "Appended article: Wanita Diduga ODGJ Ditemukan Tewas Tanpa Busana di Cileungsi Bogor\n",
            "Appended article: Pesan Kapolri ke Calon Perwira: Jaga Sinergitas, Mengabdi Sepenuh Jiwa Raga\n",
            "Appended article: Kemenhub Perketat Pengawasan Usai Boeing Ngaku Salah dalam Kecelakaan 737 MAX\n",
            "Appended article: Heru Budi Jawab soal 'Dikerjain' Jelang Pilgub Jakarta\n",
            "Appended article: Damkar Depok Klarifikasi soal Viral Saldo Kartu Isi Bensin Truk Rp 75 Ribu\n",
            "Appended article: Macet di Jalan Layang Gelong Jakbar, Ternyata Ada Sopir Takut Ketinggian\n",
            "Appended article: Ketua Asosiasi Bupati Se-RI Curhat ke Jokowi soal Anggaran Jalan Belum Turun\n",
            "Appended article: Polri Pakai Alat Analisis Komposisi Tubuh-Tes MMPI di Seleksi Akhir Akpol\n",
            "Appended article: Heru soal Jakarta Semrawut: Jangan Bebankan ke Saya Dong, Itu PR Berkelanjutan\n",
            "Appended article: 4 Fakta Tabrakan Maut Alya Vs Fortuner Buntut Terobos Lampu Merah\n",
            "Appended article: Cerita Heru Budi Benahi Data 1 Juta Kartu Jakarta Pintar Tak Tepat Sasaran\n",
            "Appended article: Heru Budi Ungkap Alasan LRT Tak Sampai Karawang dan MRT Tak Sampai BSD\n",
            "Appended article: Kesaksian Pengemudi Selamat dari Jepitan Bus dan Truk di Tol Cipularang\n",
            "Appended article: MenPAN-RB dan Menteri Singapura Bahas Akselerasi Layanan Publik Digital\n",
            "Appended article: Polisi Panggil Tiko Aryawardhana Terkait Dugaan Penggelapan Rp 6,9 M Besok\n",
            "Appended article: PDIP Bangkalan Benarkan Rumah Kadernya Anggota DPRD Jatim Digeledah KPK\n",
            "Scraped 20 articles from Detik\n",
            "Crawling page 1\n",
            "Found 15 articles on page: https://www.kompas.com/tag/politik\n",
            "Appended article: Wapres Ma'ruf: yang Ingin Perbaiki KPK Jangan Teriak Saja!\n",
            "Appended article: PDI-P Terbuka Bertemu Kaesang Pangarep\n",
            "Appended article: Pembangunan IKN Akan Disetop Sementara, Apa Sebabnya?\n",
            "Appended article: Kaesang Berpotensi Menang di Jateng, Ini 5 Faktor Pentingnya\n",
            "Appended article: Iran Geram, AS Sebut Pemilu Iran Tak Adil dan Tak Bebas\n",
            "Appended article: Perbedaan Presiden Iran Dahulu dan Sekarang, dari Kalangan Ulama dan Dokter\n",
            "Appended article: [Full] Baleg DPR Bahas Cepat RUU Wantimpres, Ini Kebijakan yang Diubah\n",
            "Appended article: Pengamat Sebut Tangerang Selatan Jadi Salah Satu Wilayah yang Diperebutkan Partai Politik\n",
            "Appended article: Teheran Geram! AS Ikut Campur Komentari Pilpres Iran\n",
            "Appended article: Datang ke Medan, Menko Polhukam Cek Kesiapan Pilkada 2024 di Sumatera\n",
            "Appended article: Dukungan PKS untuk Anies pada Pilkada Jakarta Dianggap Rezeki Politik, Bukan Hambatan\n",
            "Appended article: Pesan Gibran soal Pilkada 2024: Kaesang Jangan di Jakarta Lah\n",
            "Appended article: Pilih Dunia Politik, Jeje Govinda Tolak Tawaran Raffi Ahmad Jadi Komisaris RANS\n",
            "Appended article: PKS Usul Semua Parpol Dapat Kursi Pimpinan DPR, Bamsoet: Saya Menyambut Baik\n",
            "Appended article: Jawab Megawati, KPK Klaim Tak Targetkan Afiliasi Politik Tersangka Korupsi\n",
            "Scraped 15 articles from Kompas\n",
            "Crawling page 1\n",
            "Found 19 articles on page: https://www.suara.com/tag/politik\n",
            "Appended article: ASN Lampung Masuk 10 Besar Tidak Netral, Bawaslu: Politik Uang Mengintai Pilkada 2024\n",
            "Appended article: Potret Kaesang Pakai Kimono Batik saat Temui Presiden PKS\n",
            "Appended article: Silaturahmi Kebangsaan, Bambang Soesatyo Temui Presiden PKS\n",
            "Appended article: Popularitas Kaesang Disebut Paling Tinggi Di Jateng, Jokowi Jadi Faktor Utama\n",
            "Appended article: Survei Indikator Politik: Kaesang Ungguli Irjen Luthfi Di Pilkada Jateng\n",
            "Appended article: Survei Pilkada Jabar 2024: Ridwan Kamil Masih Favorit, Tapi Komeng Ikut Ngekor\n",
            "Appended article: Siapa Kader Gerindra yang Bertarung di Pilbup Bogor? Sosok Ini Disebut Punya Nilai Tinggi\n",
            "Appended article: Ketua ICMI: Sistem Politik Kita Makin Tidak Inklusif\n",
            "Appended article: Pengamat Beberkan Kemungkinan Golkar Tak Dapat Koalisi di Pilgub Banten\n",
            "Appended article: Bukan Pengusaha! Terungkap Pekerjaan yang Diincar Azriel Hermansyah Setelah Nikahi Sarah Menzel\n",
            "Appended article: Bila Ridwan Kamil ke Jakarta, Dedi Mulyadi Bakal Dapat Limpahan Suara di Pilgub Jabar\n",
            "Appended article: Ada Dedi Mulyadi di Jabar, Golkar Masih Tarik Ulur Bawa Ridwan Kamil ke Jakarta?\n",
            "Appended article: Momen Ridwan Kamil Skakmat Panji Pragiwaksono Saat Singgung Soal Politik Dinasti\n",
            "Appended article: Sosok Jusuf Wanandi, Petinggi Total Politik Sempat Sebut Anies Tak Bisa Kerja\n",
            "Appended article: Soal Aturan Usia Muda Calon Kepala Daerah, Mardani Ali Sera: Kasihan Publik\n",
            "Appended article: Mengulik Sepak Terjang Pujiyono Suwandi, Ketua Komisi Kejaksaan yang Dirumorkan Maju Pilkada Boyolali\n",
            "Appended article: Tobat Politik! Angelina Sondakh Ungkap Alasan Tak Mau Lagi Jadi Pejabat\n",
            "Appended article: Apa Itu Politik Uang? Ada 4 Kategori Dalam Pilkada\n",
            "Appended article: Pilkada Serentak 27 November 2024, Masyarakat Diminta Tidak Memilih Karena Uang\n",
            "Scraped 19 articles from Suara\n",
            "Crawling page 1\n",
            "Found 15 articles on page: https://www.antaranews.com/politik\n",
            "Appended article: Muzani pastikan komitmen pemerintahan Prabowo cegah malaadministrasi\n",
            "Appended article: PDI-P Sumut silaturahmi ke PKS Sumut buka peluang koalisi Pilkada 2024\n",
            "Appended article: Ridwan Kamil: Belum ada keputusan maju di Pilkada Jabar atau DKI\n",
            "Appended article: Demokrat umumkan calon untuk Pilkada di Riau, Sulut, dan Sultra\n",
            "Appended article: Seleksi Akpol Polri gunakan teknologi digital cek kondisi tubuh\n",
            "Appended article: Pengamat: Komunikasi media sosial tingkatkan elektabilitas Kaesang\n",
            "Appended article: Golkar usul Bupati Asahan jadi cawagub Bobby di Pilkada Sumut\n",
            "Appended article: Airlangga buka suara  soal baliho Ahmad Lutfi-Gus Yasin\n",
            "Appended article: UII: Aksi boikot produk Israel jangan untuk persaingan bisnis\n",
            "Appended article: Komisi C DPRD Surabaya minta studi kelayakan PSN dilakukan detail\n",
            "Appended article: Sumbar rawan pelanggaran Pilkada, KPU sarankan perkuat konsolidasi\n",
            "Appended article: DPRD: verifikasi adminduk mendetil cegah kesalahan usulan blokir KK\n",
            "Appended article: KPU jadikan Indeks Kerawanan Pemilu Sumbar sebagai peringatan dini\n",
            "Appended article: Demokrat tambah daftar partai KIM resmi dukung Khofifah-Emil di Jatim\n",
            "Appended article: Bambang-Syiraj terima rekomendasi PPP maju Pilkada Dompu\n",
            "Scraped 15 articles from Antara\n",
            "Total articles collected: 79\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Load JSON file\n",
        "with open('news_websites.json') as json_file:\n",
        "    websites = json.load(json_file)['websites']\n",
        "\n",
        "def get_soup(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "def parse_article(link, website):\n",
        "    try:\n",
        "        soup = get_soup(link)\n",
        "        content_div = soup.find(class_=website['content_class'])\n",
        "        content = ' '.join(p.get_text(strip=True) for p in content_div.find_all('p')) if content_div else 'No content found'\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing article {link}: {e}\")\n",
        "        return 'No content found'\n",
        "\n",
        "def parse_page(url, website):\n",
        "    soup = get_soup(url)\n",
        "    news_data = []\n",
        "    articles = soup.find_all(website['article_tag'], class_=website['article_class'])\n",
        "    print(f\"Found {len(articles)} articles on page: {url}\")\n",
        "\n",
        "    for article in articles:\n",
        "        title_tag = article.find(class_=website.get('title_class', None))\n",
        "        if title_tag:\n",
        "            title = title_tag.get_text(strip=True)\n",
        "        else:\n",
        "            title_tag = article.find('a')\n",
        "            title = title_tag['title'] if 'title' in title_tag.attrs else title_tag.get_text(strip=True)\n",
        "\n",
        "        link_tag = article.find('a')\n",
        "        link = link_tag['href'] if link_tag else 'No link found'\n",
        "\n",
        "        date_tag = article.find(class_=website['date_class'])\n",
        "        date = date_tag.get_text(strip=True) if date_tag else 'No date found'\n",
        "\n",
        "        image_tag = article.find('img')\n",
        "        if(website['name'] == \"Antara\"):\n",
        "            image = image_tag['data-src'] if image_tag else 'No image found'\n",
        "        else:\n",
        "            image = image_tag['src'] if image_tag else 'No image found'\n",
        "\n",
        "        content = parse_article(link, website) if link != 'No link found' else 'No content found'\n",
        "\n",
        "        news_data.append({\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            \"image\" : image,\n",
        "            'date': date,\n",
        "            'content': content,\n",
        "            'is_fake': 0,\n",
        "            'media_bias': website['platform']\n",
        "        })\n",
        "        print(f\"Appended article: {title}\")\n",
        "\n",
        "    return news_data\n",
        "\n",
        "def get_all_articles(base_url, website, max_pages=1):\n",
        "    articles = []\n",
        "    next_page = base_url\n",
        "    current_page = 1\n",
        "\n",
        "    while next_page and current_page <= max_pages:\n",
        "        print(f\"Crawling page {current_page}\")\n",
        "        articles.extend(parse_page(next_page, website))\n",
        "        soup = get_soup(next_page)\n",
        "        if website['name'] == 'Antara':\n",
        "            next_page = f\"{base_url}/{current_page + 1}\"\n",
        "        elif website['name'] == 'Suara':\n",
        "            next_page = f\"{base_url}?page={current_page + 1}\"\n",
        "        elif website['name'] == 'Detik':\n",
        "            next_page = f\"{base_url}/{current_page + 1}\"\n",
        "        elif website['name'] == 'Tribunnews':\n",
        "            break\n",
        "        else:\n",
        "            next_button = soup.find(class_=website['next_page'])\n",
        "            next_page = next_button[\"href\"] if next_button else None\n",
        "        current_page += 1\n",
        "        time.sleep(2)\n",
        "    return articles\n",
        "\n",
        "def crawlerGeneral():\n",
        "    all_news = []\n",
        "    for website in websites:\n",
        "        try:\n",
        "            base_url = website['url']\n",
        "            scraped_news = get_all_articles(base_url, website)\n",
        "            print(f\"Scraped {len(scraped_news)} articles from {website['name']}\")\n",
        "            all_news.extend(scraped_news)\n",
        "            time.sleep(2) \n",
        "        except requests.HTTPError as e:\n",
        "            print(f\"Failed to scrape {website['name']}: {e}\")\n",
        "\n",
        "    with open('scraped_news.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_news, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Total articles collected: {len(all_news)}\")\n",
        "    return all_news\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    crawlerGeneral()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GDmxLKCbVL4"
      },
      "source": [
        "# Crawling news data using user input or by topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Y25gLE_sbaM3",
        "outputId": "87fc6da1-9713-41ed-b93e-956d64c73d06"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'selenium'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e96dc670b164>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Import Required Libraries\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Define User-Agent for Requests\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Load JSON file\n",
        "with open('news_websites.json') as json_file:\n",
        "    websites = json.load(json_file)['websites']\n",
        "\n",
        "# Function to Get BeautifulSoup Object from URL\n",
        "def get_soup(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Function to Parse Individual Article\n",
        "def parse_article(link, website):\n",
        "    try:\n",
        "        soup = get_soup(link)\n",
        "        content_div = soup.find(class_=website['content_class'])\n",
        "        content = ' '.join(p.get_text(strip=True) for p in content_div.find_all('p')) if content_div else 'No content found'\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing article {link}: {e}\")\n",
        "        return 'No content found'\n",
        "\n",
        "# Function to Parse Articles on a Page\n",
        "def parse_page(url, website):\n",
        "    soup = get_soup(url)\n",
        "    news_data = []\n",
        "    articles = soup.find_all(website['article_tag'], class_=website['article_class'])\n",
        "    print(f\"Found {len(articles)} articles on page: {url}\")\n",
        "    for article in articles:\n",
        "        title_tag = article.find(class_=website.get('title_class', None))\n",
        "        if title_tag:\n",
        "            title = title_tag.get_text(strip=True)\n",
        "        else:\n",
        "            title_tag = article.find('a')\n",
        "            if title_tag:\n",
        "                title = title_tag['title'] if 'title' in title_tag.attrs else title_tag.get_text(strip=True)\n",
        "            else:\n",
        "                title = 'No title found'\n",
        "        link_tag = article.find('a')\n",
        "        link = link_tag['href'] if link_tag else 'No link found'\n",
        "\n",
        "        date_tag = article.find(class_=website['date_class'])\n",
        "        date = date_tag.get_text(strip=True) if date_tag else 'No date found'\n",
        "\n",
        "        content = parse_article(link, website) if link != 'No link found' else 'No content found'\n",
        "\n",
        "        news_data.append({\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            'date': date,\n",
        "            'content': content,\n",
        "            'is_fake': 0,\n",
        "            'media_bias': website['platform']\n",
        "        })\n",
        "        print(f\"Appended article: {title}\")\n",
        "\n",
        "    return news_data\n",
        "\n",
        "# Function to Get All Articles from a Website\n",
        "def get_all_articles(base_url, website, max_pages=2):\n",
        "    articles = []\n",
        "    next_page = base_url\n",
        "    current_page = 1\n",
        "\n",
        "    while next_page and current_page <= max_pages:\n",
        "        print(f\"Crawling page {current_page}\")\n",
        "        articles.extend(parse_page(next_page, website))\n",
        "        soup = get_soup(next_page)\n",
        "        if website['name'] == 'Antara':\n",
        "            next_page = f\"{base_url}&page={current_page + 1}\"\n",
        "        elif website['name'] == 'Suara':\n",
        "            next_page = f\"{base_url}?page={current_page + 1}\"\n",
        "        elif website['name'] == 'Detik':\n",
        "            next_page = f\"{base_url}&page={current_page + 1}\"\n",
        "        else:\n",
        "            next_button = soup.find(class_=website['next_page'])\n",
        "            next_page = next_button[\"href\"] if next_button else None\n",
        "        current_page += 1\n",
        "        time.sleep(2)\n",
        "    return articles\n",
        "\n",
        "# Function to Execute the Crawling Process\n",
        "def main():\n",
        "    all_news = []\n",
        "    topik = input(\"Masukkan Topik: \")\n",
        "    for website in websites:\n",
        "        try:\n",
        "            base_url = website['url'] + topik\n",
        "            scraped_news = get_all_articles(base_url, website)\n",
        "            print(f\"Scraped {len(scraped_news)} articles from {website['name']}\")\n",
        "            all_news.extend(scraped_news)\n",
        "            time.sleep(2)  # Respectful delay to avoid overwhelming the server\n",
        "        except requests.HTTPError as e:\n",
        "            print(f\"Failed to scrape {website['name']}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(all_news)\n",
        "    print(f\"Total articles collected: {len(all_news)}\")\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv('scraped_news.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONkGhgereeCA"
      },
      "source": [
        "**TESTING SUCCESSFUL FOR CRAWLING WITH USER INPUT**\n",
        "\n",
        "* For crawling news data based on user input use the coding below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xljXGKgledPs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Load JSON file\n",
        "with open('news_websites.json') as json_file:\n",
        "    websites = json.load(json_file)['websites']\n",
        "\n",
        "def get_soup(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "def parse_article(link, website):\n",
        "    try:\n",
        "        soup = get_soup(link)\n",
        "        content_div = soup.find(class_=website['content_class'])\n",
        "        content = ' '.join(p.get_text(strip=True) for p in content_div.find_all('p')) if content_div else 'No content found'\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing article {link}: {e}\")\n",
        "        return 'No content found'\n",
        "\n",
        "def parse_page(url, website):\n",
        "    news_data = []\n",
        "\n",
        "    if website['name'] == 'CNN Indonesia':\n",
        "        driver = webdriver.Chrome()\n",
        "        driver.get(url)\n",
        "        \n",
        "        try:\n",
        "            WebDriverWait(driver, 20).until(\n",
        "                EC.invisibility_of_element_located((By.CLASS_NAME, 'animate-pulse'))\n",
        "            )\n",
        "            \n",
        "            content = driver.page_source\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "            \n",
        "            articles = soup.find_all('article')\n",
        "            \n",
        "            for article in articles:\n",
        "                title = article.find('h2').get_text(strip=True) if article.find('h2') else 'No Title'\n",
        "                date_tag = article.find('span', class_='text-xs text-cnn_black_light3')\n",
        "                date = date_tag.get_text(strip=True) if date_tag else 'No date found'\n",
        "\n",
        "                link_tag = article.find('a')\n",
        "                link = link_tag['href'] if link_tag else 'No link found'\n",
        "\n",
        "                image_tag = article.find('img')\n",
        "                image = image_tag['src'] if image_tag else 'No image found'\n",
        "                content = parse_article(link, website) if link != 'No link found' else 'No content found'\n",
        "\n",
        "                news_data.append({\n",
        "                    'title': title,\n",
        "                    'content': content,\n",
        "                    'date': date,\n",
        "                    'link': link,\n",
        "                    'image': image,\n",
        "                    'is_fake': 0,\n",
        "                    'media_bias': 'CNN Indonesia'\n",
        "                })\n",
        "        \n",
        "        finally:\n",
        "            driver.quit()\n",
        "\n",
        "    else:\n",
        "        soup = get_soup(url)\n",
        "        articles = soup.find_all(website['article_tag'], class_=website['article_class'])\n",
        "        print(f\"Found {len(articles)} articles on page: {url}\")\n",
        "        \n",
        "        for article in articles:\n",
        "            title_tag = article.find(class_=website.get('title_class', None))\n",
        "            if title_tag:\n",
        "                title = title_tag.get_text(strip=True)\n",
        "            else:\n",
        "                title_tag = article.find('a')\n",
        "                if title_tag:\n",
        "                    title = title_tag['title'] if 'title' in title_tag.attrs else title_tag.get_text(strip=True)\n",
        "                else:\n",
        "                    title = 'No title found'\n",
        "            link_tag = article.find('a')\n",
        "            link = link_tag['href'] if link_tag else 'No link found'\n",
        "\n",
        "            date_tag = article.find(class_=website['date_class'])\n",
        "            date = date_tag.get_text(strip=True) if date_tag else 'No date found'\n",
        "\n",
        "            image_tag = article.find('img')\n",
        "            if(website['name'] == 'Antara'):\n",
        "                image = image_tag['data-src'] if image_tag else 'No image found'\n",
        "            elif(website['name'] == 'Detik'):\n",
        "                image = image_tag['src'] if image_tag else 'No image found'\n",
        "\n",
        "            content = parse_article(link, website) if link != 'No link found' else 'No content found'\n",
        "\n",
        "            news_data.append({\n",
        "                'title': title,\n",
        "                'link': link,\n",
        "                'date': date,\n",
        "                'content': content,\n",
        "                'image': image,\n",
        "                'is_fake': 0,\n",
        "                'media_bias': website['platform']\n",
        "            })\n",
        "            print(f\"Appended article: {title}\")\n",
        "\n",
        "    return news_data\n",
        "\n",
        "def get_all_articles(base_url, website, max_pages=2):\n",
        "    articles = []\n",
        "    next_page = base_url\n",
        "    current_page = 1\n",
        "\n",
        "    while next_page and current_page <= max_pages:\n",
        "        print(f\"Crawling page {current_page}\")\n",
        "        articles.extend(parse_page(next_page, website))\n",
        "        if website['name'] in ['Antara', 'Suara', 'Detik']:\n",
        "            next_page = f\"{base_url}&page={current_page + 1}\"\n",
        "        else:\n",
        "            soup = get_soup(next_page)\n",
        "            next_button = soup.find(class_=website['next_page'])\n",
        "            next_page = next_button[\"href\"] if next_button else None\n",
        "        current_page += 1\n",
        "        time.sleep(2)\n",
        "    return articles\n",
        "\n",
        "def crawlerWithTopik(topik):\n",
        "    all_news = []\n",
        "    for website in websites:\n",
        "        try:\n",
        "            base_url = website['url'] + topik\n",
        "            scraped_news = get_all_articles(base_url, website)\n",
        "            print(f\"Scraped {len(scraped_news)} articles from {website['name']}\")\n",
        "            all_news.extend(scraped_news)\n",
        "            time.sleep(2)\n",
        "        except requests.HTTPError as e:\n",
        "            print(f\"Failed to scrape {website['name']}: {e}\")\n",
        "\n",
        "    with open('scraped_news.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_news, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Total articles collected: {len(all_news)}\")\n",
        "    return all_news\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    topik = input(\"Masukkan Topik: \")\n",
        "    crawlerWithTopik(topik)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPkIStMfAh/1Y6U631sW/sa",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
